{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, ReLU, LeakyReLU, Reshape, Conv2DTranspose, Conv2D, BatchNormalization, Dropout, Flatten, InputLayer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "import numpy as np\n",
    "import os\n",
    "from src import ImageController, Evaluator, SaveController\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspect the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_dir_url = 'data'\n",
    "\n",
    "ImageController.plot_random_images(image_dir_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_images = ImageController.load_all_images(image_dir_url).astype('float32')\n",
    "# Images should be normalized to a range of -1 to 1 in order to match the output of the generator.\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "\n",
    "BUFFER_SIZE = len(train_images)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# This creates an iterator object which serves batches of size BATCH_SIZE consisting of random normalized images once iterated over.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(tensors=train_images).shuffle(buffer_size=BUFFER_SIZE).batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print('The shape of the dataset:')\n",
    "train_images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The kernel initializer defines how the weights in a neural network layer should be initialized. This has a direct effect on convergence speed. A recommended Distribution for training DCGANs is the following:\n",
    "kernel_initializer = RandomNormal(stddev=0.02)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_generator(noise_dimensions):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(noise_dimensions,)))\n",
    "\n",
    "    # First layer has to be a dense layer which expands the noise vector. This has to be done to create the required 16.384 values which after being reshaped form the input feature maps for the first deconvolutional layer.\n",
    "    model.add(Dense(units=4 * 4 * 1024, kernel_initializer=kernel_initializer, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Reshape(target_shape=(4, 4, 1024)))\n",
    "\n",
    "    # The output feature maps of the first deconvolutional layer have a shape of (8, 8, 512). This could be achieved by the stride set to 2 and padding set to 'same' which basically doubles the size of the image (i.e. the pixels).\n",
    "    model.add(Conv2DTranspose(filters=512, kernel_size=(5, 5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    # After the second one there are 256 feature maps with a size of 16x16 left.\n",
    "    model.add(Conv2DTranspose(filters=256, kernel_size=(5, 5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    # After the third block there are only 128 feature maps with a size of 32x32 left.\n",
    "    model.add(Conv2DTranspose(filters=128, kernel_size=(5, 5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    # After the last deconvolutional layer the output has a size of 64x64x3 (RGB image with a width and height of 64px).\n",
    "    model.add(Conv2DTranspose(filters=3, kernel_size=(5, 5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    # Tanh activation function is used to scale the values between -1 and 1 (same range as the preprocessed real images).\n",
    "    model.add(Activation(activation='tanh'))\n",
    "\n",
    "    assert model.output_shape == (None, 64, 64, 3)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Definition of the dimensionality of the noise vector:\n",
    "NOISE_DIMENSIONS = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = create_generator(NOISE_DIMENSIONS)\n",
    "\n",
    "print('The topography of the generator\\'s model:')\n",
    "generator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Here an example of the output from the untrained generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Definition of how many sample images should be plotted and saved every epoch to monitor the progress:\n",
    "EXAMPLES_TO_GENERATE = 16\n",
    "\n",
    "# A seed is set so that each time the program is restarted the same vector is created. This is essential for reproducability.\n",
    "tf.random.set_seed(0)\n",
    "# The seed is a pseudo-random vector used throughout the training to generate images and compare the generator's output between different training epochs.\n",
    "seed = tf.random.normal(shape=[EXAMPLES_TO_GENERATE, NOISE_DIMENSIONS])\n",
    "\n",
    "print('The shape of the seed:')\n",
    "seed.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ImageController.generate_and_plot_images(generator, seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the discriminator model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    model = Sequential()\n",
    "\n",
    "    # RGB image with a width of 64 and a height of 64\n",
    "    model.add(InputLayer(input_shape=(64, 64, 3)))\n",
    "\n",
    "    # The output of the first convolutional layer is 64 feature maps with a width of 32 and a height of 32 due to stride set to 2 and padding set to 'same'.\n",
    "    model.add(Conv2D(filters=64, kernel_size=(5,5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    # The second one produces 128 feature maps which are 16x16.\n",
    "    model.add(Conv2D(filters=128, kernel_size=(5,5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    # The third one produces 256 with a shape of (8, 8).\n",
    "    model.add(Conv2D(filters=256, kernel_size=(5,5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    # The last convolutional layer has an output of (4, 4, 512).\n",
    "    model.add(Conv2D(filters=512, kernel_size=(5,5), kernel_initializer=kernel_initializer, strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "\n",
    "    # Matrices have to be flattened to a vector to be able to add the output layer which is fully connected.\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=1, kernel_initializer=kernel_initializer, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    # Sigmoid function is used to scale the output to a range of 0 to 1 which is the probability of an image being fake or real.\n",
    "    model.add(Activation(activation='sigmoid'))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discriminator = create_discriminator()\n",
    "\n",
    "print('The topography of the discriminator\\'s model:')\n",
    "discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Losses definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Binary crossentropy is used for binary classification problems (i.e. real vs. fake).\n",
    "binary_crossentropy = BinaryCrossentropy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The loss of the discriminator consists of the sum of the loss of the real image vs. its prediction and the loss of the fake image vs. its prediction.\n",
    "def calc_discriminator_loss(real_output, fake_output):\n",
    "    y_true_positive = np.ones_like(real_output).flatten()\n",
    "    # Noise is added to the true values to prevent overfitting\n",
    "    y_true_positive = flip_labels(y_true_positive, 0.05)\n",
    "    # Labels are smoothed out to prevent overfitting\n",
    "    y_true_positive = smooth_out_labels(y_true_positive, 0.3)\n",
    "    y_true_positive = np.reshape(y_true_positive, (-1, 1))\n",
    "\n",
    "    y_true_negative = np.zeros_like(fake_output).flatten()\n",
    "    # Noise is added to the true values to prevent overfitting\n",
    "    y_true_negative = flip_labels(y_true_negative, 0.05)\n",
    "    # Labels are smoothed out to prevent overfitting\n",
    "    y_true_negative = smooth_out_labels(y_true_negative, 0.3)\n",
    "    y_true_negative = np.reshape(y_true_negative, (-1, 1))\n",
    "\n",
    "    real_loss = binary_crossentropy(y_true=y_true_positive, y_pred=real_output)\n",
    "    fake_loss = binary_crossentropy(y_true=y_true_negative, y_pred=fake_output)\n",
    "\n",
    "    return real_loss, fake_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If the generator does a good job the discriminator will classify the image as real  (i.e. as 1). Therefore for generator loss y_true is 1.\n",
    "def calc_generator_loss(fake_output):\n",
    "    return binary_crossentropy(y_true=tf.ones_like(fake_output), y_pred=fake_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function makes the true values of the discriminator (i.e. 0 or 1) dynamic (Randomly between 0 and smoothness_range or smoothness_range and 1). This is really useful to ensure that the discriminator is not overconfident with its predictions.\n",
    "def smooth_out_labels(labels, smoothness_range):\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 0:\n",
    "            labels[i] = np.random.random() * smoothness_range\n",
    "        elif label == 1:\n",
    "            labels[i] = 1 - np.random.random() * smoothness_range\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# With probability flip_probability noise is added to the true values. This is really useful to ensure that the discriminator is not overconfident with its predictions.\n",
    "def flip_labels(labels, flip_probability):\n",
    "    for i, label in enumerate(labels):\n",
    "        if flip_probability > np.random.random():\n",
    "            labels[i] = 0 if label == 1 else 1\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizers definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adam optimizer with a momentum of 0.5 and a learning rate of 0.0002 is recommended for the training of DCGANs.\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00003, beta_1=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training functions definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_iteration(images_batch):\n",
    "    noise = tf.random.normal(shape=[BATCH_SIZE, NOISE_DIMENSIONS])\n",
    "\n",
    "    with tf.GradientTape() as generator_tape, tf.GradientTape() as discriminator_tape:\n",
    "        generated_images_batch = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images_batch)\n",
    "        fake_output = discriminator(generated_images_batch)\n",
    "\n",
    "        generator_loss = calc_generator_loss(fake_output)\n",
    "        discriminator_real_loss, discriminator_fake_loss = calc_discriminator_loss(real_output, fake_output)\n",
    "        discriminator_total_loss = discriminator_real_loss + discriminator_fake_loss\n",
    "\n",
    "    discriminator_gradients = discriminator_tape.gradient(target=discriminator_total_loss, sources=discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    generator_gradients = generator_tape.gradient(target=generator_loss, sources=generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "\n",
    "    return real_output, fake_output, discriminator_real_loss, discriminator_fake_loss, discriminator_total_loss, generator_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(image_dataset, epochs, generated_images_dir_url: str, generated_figures_dir_url: str):\n",
    "    training_start = time.time()\n",
    "    print('\\nTRAINING STARTED')\n",
    "\n",
    "    # Lists with all the metrics.\n",
    "    generator_losses = []\n",
    "    discriminator_real_losses = []\n",
    "    discriminator_fake_losses = []\n",
    "    discriminator_total_losses = []\n",
    "    discriminator_real_accuracies = []\n",
    "    discriminator_fake_accuracies = []\n",
    "    discriminator_total_accuracies = []\n",
    "\n",
    "    for epoch in epochs:\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        for i, image_batch in enumerate(image_dataset):\n",
    "            real_output, fake_output, discriminator_real_loss, discriminator_fake_loss, discriminator_total_loss, generator_loss = train_iteration(image_batch)\n",
    "\n",
    "            # At each epochs' last iteration the current metrics should be saved for future analysis\n",
    "            if i == len(image_dataset) - 1:\n",
    "                generator_losses.append(generator_loss)\n",
    "                discriminator_real_losses.append(discriminator_real_loss)\n",
    "                discriminator_fake_losses.append(discriminator_fake_loss)\n",
    "                discriminator_total_losses.append(discriminator_total_loss)\n",
    "\n",
    "                y_true_positive = np.ones_like(real_output)\n",
    "                y_true_negative = np.zeros_like(fake_output)\n",
    "                binary_accuracy = BinaryAccuracy()\n",
    "                binary_accuracy.update_state(y_true_positive, real_output)\n",
    "                discriminator_real_accuracy = binary_accuracy.result().numpy()\n",
    "                binary_accuracy.reset_states()\n",
    "                binary_accuracy.update_state(y_true_negative, fake_output)\n",
    "                discriminator_fake_accuracy = binary_accuracy.result().numpy()\n",
    "                binary_accuracy.reset_states()\n",
    "                binary_accuracy.update_state(np.vstack((y_true_positive, y_true_negative)), np.vstack((real_output, fake_output)))\n",
    "                discriminator_total_accuracy = binary_accuracy.result().numpy()\n",
    "                binary_accuracy.reset_states()\n",
    "                discriminator_real_accuracies.append(discriminator_real_accuracy)\n",
    "                discriminator_fake_accuracies.append(discriminator_fake_accuracy)\n",
    "                discriminator_total_accuracies.append(discriminator_total_accuracy)\n",
    "\n",
    "        ImageController.generate_and_save_images(generator, seed, os.path.join(generated_images_dir_url, f'images-at-epoch-{epoch}'))\n",
    "        SaveController.save_checkpoint_weights(epoch, generator, discriminator, generator_checkpoints_dir_url, discriminator_checkpoints_dir_url, generator_losses[-1], discriminator_total_losses[-1])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(f'\\nTime for epoch {epoch}: {epoch_end - epoch_start} sec')\n",
    "        print(f'Generator loss: {generator_losses[-1]}; Discriminator real loss: {discriminator_real_losses[-1]}; Discriminator fake loss: {discriminator_fake_losses[-1]}; Discriminator total loss: {discriminator_total_losses[-1]};')\n",
    "        print(f'Discriminator real accuracy: {discriminator_real_accuracies[-1]}; Discriminator fake accuracy: {discriminator_fake_accuracies[-1]}; Discriminator total accuracy: {discriminator_total_accuracies[-1]}')\n",
    "\n",
    "    training_end = time.time()\n",
    "\n",
    "    print(f'\\nTotal time: {(training_end - training_start)/60} min\\n')\n",
    "    print('TRAINING COMPLETED')\n",
    "\n",
    "    Evaluator.plot_and_save_losses(generator_losses, discriminator_real_losses, discriminator_fake_losses, discriminator_total_losses, epochs, generated_figures_dir_url)\n",
    "    Evaluator.plot_and_save_discriminator_accuracies(discriminator_real_accuracies, discriminator_fake_accuracies, discriminator_total_accuracies, epochs, generated_figures_dir_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "end_epoch = 1000\n",
    "\n",
    "# URLs\n",
    "generator_checkpoints_dir_url = os.path.join('assets', 'checkpoints', 'generator')\n",
    "discriminator_checkpoints_dir_url = os.path.join('assets', 'checkpoints', 'discriminator')\n",
    "# Directory which stores all generated media in its subdirectories (i.e. the generated Pokémon from the seed, the gif which shows the progress throughout all epochs and the evaluation figures constructed at the end of the training)\n",
    "generated_media_dir_url = os.path.join('assets', 'generated-media')\n",
    "# Output URL for the generator once training is over and should_save_generator is set to True\n",
    "trained_generator_model_dir_url = '../public'\n",
    "\n",
    "# Controls whether previously saved checkpoint files should be used or not\n",
    "should_use_checkpoint_files = True\n",
    "# Controls whether the generator model should be saved at the end of the training at the specified path for future predictions\n",
    "should_save_generator = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Actual training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Depending on the values of the hyperparameters and possibly training done before different initial values are set.\n",
    "do_checkpoint_files_exist = SaveController.do_checkpoint_files_exist(generator_checkpoints_dir_url, discriminator_checkpoints_dir_url)\n",
    "\n",
    "if do_checkpoint_files_exist:\n",
    "    print('Found previously saved checkpoint files')\n",
    "    if should_use_checkpoint_files:\n",
    "        print(f'Will be using previous checkpoint files because should_use_checkpoint_files is set to: {should_use_checkpoint_files}')\n",
    "        last_created_generated_media_subdir_url = SaveController.get_last_created_file_or_subdirectory_url(generated_media_dir_url)\n",
    "        generated_images_dir_url = os.path.join(last_created_generated_media_subdir_url, 'images')\n",
    "        generated_gifs_dir_url = os.path.join(last_created_generated_media_subdir_url, 'gifs')\n",
    "        generated_figures_dir_url = os.path.join(last_created_generated_media_subdir_url, 'figures')\n",
    "        last_completed_epoch = SaveController.load_latest_checkpoint_files(generator, discriminator, generator_checkpoints_dir_url, discriminator_checkpoints_dir_url)\n",
    "        start_epoch = last_completed_epoch + 1\n",
    "    else:\n",
    "        print(f'Will not be using previous checkpoint files because should_use_checkpoint_files is set to: {should_use_checkpoint_files}')\n",
    "        SaveController.delete_checkpoint_files(generator_checkpoints_dir_url, discriminator_checkpoints_dir_url)\n",
    "        print('Previous checkpoint files have been deleted')\n",
    "\n",
    "        generated_images_dir_url, generated_gifs_dir_url, generated_figures_dir_url = SaveController.get_new_generated_media_subdir_urls(generated_media_dir_url)\n",
    "        SaveController.create_generated_media_dirs(generated_images_dir_url, generated_gifs_dir_url, generated_figures_dir_url)\n",
    "        start_epoch = 1\n",
    "else:\n",
    "    print('Did not find any previously saved checkpoint files')\n",
    "    generated_images_dir_url, generated_gifs_dir_url, generated_figures_dir_url = SaveController.get_new_generated_media_subdir_urls(generated_media_dir_url)\n",
    "    SaveController.create_generated_media_dirs(generated_images_dir_url, generated_gifs_dir_url, generated_figures_dir_url)\n",
    "    SaveController.create_checkpoint_dirs(generator_checkpoints_dir_url, discriminator_checkpoints_dir_url)\n",
    "    start_epoch = 1\n",
    "\n",
    "\n",
    "print(f'Starting training from epoch {start_epoch}. Remaining epochs: {end_epoch - start_epoch + 1}')\n",
    "#train(train_dataset, range(start_epoch, end_epoch + 1), generated_images_dir_url, generated_figures_dir_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A GIF which shows the whole training progress is generated and saved after the completion of the training\n",
    "ImageController.create_and_save_gif(image_dir_url=generated_images_dir_url, output_url=os.path.join(generated_gifs_dir_url, 'pokegan.gif'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If the generator should be saved for future production use or not.\n",
    "if should_save_generator:\n",
    "    SaveController.save_generator_model(generator, trained_generator_model_dir_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pokegan",
   "language": "python",
   "display_name": "Python (pokegan)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}